{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/3c/c800c23068ef23dedbb2641574b24cbc6d51c7d7b7bddbc803a93d7409d3/mxnet-1.6.0-py2.py3-none-win_amd64.whl\n",
      "Collecting numpy<1.17.0,>=1.8.2 (from mxnet)\n",
      "Collecting requests<2.19.0,>=2.18.4 (from mxnet)\n",
      "  Using cached https://files.pythonhosted.org/packages/49/df/50aa1999ab9bde74656c2919d9c0c085fd2b3775fd3eca826012bef76d8c/requests-2.18.4-py2.py3-none-any.whl\n",
      "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Using cached https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.23,>=1.21.1 (from requests<2.19.0,>=2.18.4->mxnet)\n",
      "  Using cached https://files.pythonhosted.org/packages/63/cb/6965947c13a94236f6d4b8223e21beb4d576dc72e8130bd7880f600839b8/urllib3-1.22-py2.py3-none-any.whl\n",
      "Collecting idna<2.7,>=2.5 (from requests<2.19.0,>=2.18.4->mxnet)\n",
      "  Using cached https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\gitprojects\\machinelearning\\auto_ml\\auto_gluon\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\gitprojects\\machinelearning\\auto_ml\\auto_gluon\\lib\\site-packages (from requests<2.19.0,>=2.18.4->mxnet) (2020.6.20)\n",
      "Installing collected packages: numpy, urllib3, idna, requests, graphviz, mxnet\n",
      "  Found existing installation: numpy 1.19.0\n",
      "    Uninstalling numpy-1.19.0:\n",
      "      Successfully uninstalled numpy-1.19.0\n",
      "  Found existing installation: urllib3 1.25.9\n",
      "    Uninstalling urllib3-1.25.9:\n",
      "      Successfully uninstalled urllib3-1.25.9\n",
      "  Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Found existing installation: graphviz 0.14\n",
      "    Uninstalling graphviz-0.14:\n",
      "      Successfully uninstalled graphviz-0.14\n",
      "Successfully installed graphviz-0.8.4 idna-2.6 mxnet-1.6.0 numpy-1.16.6 requests-2.18.4 urllib3-1.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\gitprojects\\machinelearning\\auto_ml\\auto_gluon\\lib\\site-packages\\mxnet\\optimizer\\optimizer.py:163: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  warnings.warn('WARNING: New optimizer %s.%s is overriding '\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import autogluon as ag\n",
    "from autogluon import TabularPrediction as task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_csv(\"NTI_data/test.csv\")\n",
    "train_target = read_csv(\"NTI_data/train_target.csv\")\n",
    "transactions_test = read_csv(\"NTI_data/transactions_test.csv\")\n",
    "transactions_train = read_csv(\"NTI_data/transactions_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "#Признаки, связанные с суммой покупки\n",
    "agg_features=transactions_train.groupby('client_id')['amount_rur'].agg(['sum','mean','max']).reset_index()\n",
    "\n",
    "#Количество в каждой категории\n",
    "amount_of_purchase=transactions_train.groupby(['client_id','small_group'])['amount_rur'].count()\n",
    "counts_train=amount_of_purchase.reset_index().pivot(index='client_id', \\\n",
    "                                                      columns='small_group',values='amount_rur')\n",
    "counts_train=counts_train.fillna(0)\n",
    "counts_train.columns=['small_group_' + str(i) for i in counts_train.columns]\n",
    "\n",
    "#Сумма в каждой категории\n",
    "sum_of_group=transactions_train.groupby(['client_id','small_group'])['amount_rur'].sum()\n",
    "sum_train=sum_of_group.reset_index().pivot(index='client_id', \\\n",
    "                                                      columns='small_group',values='amount_rur')\n",
    "sum_train=sum_train.fillna(0)\n",
    "sum_train.columns=['small_group_sum_'+str(i) for i in sum_train.columns]\n",
    "\n",
    "#Минимум в каждой категории\n",
    "min_of_group=transactions_train.groupby(['client_id','small_group'])['amount_rur'].min()\n",
    "min_train=sum_of_group.reset_index().pivot(index='client_id', \\\n",
    "                                                      columns='small_group',values='amount_rur')\n",
    "min_train=min_train.fillna(0)\n",
    "min_train.columns=['small_group_min_'+str(i) for i in min_train.columns]\n",
    "\n",
    "#Максимум в каждой категории\n",
    "max_of_group=transactions_train.groupby(['client_id','small_group'])['amount_rur'].min()\n",
    "max_train=sum_of_group.reset_index().pivot(index='client_id', \\\n",
    "                                                      columns='small_group',values='amount_rur')\n",
    "max_train=max_train.fillna(0)\n",
    "max_train.columns=['small_group_max_'+str(i) for i in max_train.columns]\n",
    "\n",
    "#Средняя сумма в каждой категории\n",
    "avg_of_group=transactions_train.groupby(['client_id','small_group'])['amount_rur'].mean()\n",
    "avg_train=avg_of_group.reset_index().pivot(index='client_id', \\\n",
    "                                                      columns='small_group',values='amount_rur')\n",
    "avg_train=avg_train.fillna(0)\n",
    "avg_train.columns=['small_group_avg_'+str(i) for i in avg_train.columns]\n",
    "\n",
    "#Соединим все в одну таблицу\n",
    "counts_train=pd.merge(sum_train,counts_train,on='client_id')\n",
    "counts_train=pd.merge(avg_train,counts_train,on='client_id')\n",
    "counts_train=pd.merge(min_train,counts_train,on='client_id')\n",
    "counts_train=pd.merge(max_train,counts_train,on='client_id')\n",
    "train=pd.merge(train_target,agg_features,on='client_id')\n",
    "train=pd.merge(train,counts_train.reset_index(),on='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.head(25000)\n",
    "test_data = train.tail(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No output_directory specified. Models will be saved in: AutogluonModels/ag-20200714_155637\\\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to AutogluonModels/ag-20200714_155637\\\n",
      "Train Data Rows:    25000\n",
      "Train Data Columns: 1015\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 4\n",
      "Feature Generator processed 25000 data points with 1009 features\n",
      "Original Features:\n",
      "\tint features: 1\n",
      "\tfloat features: 1008\n",
      "\tobject features: 0\n",
      "Generated Features:\n",
      "\tint features: 0\n",
      "All Features:\n",
      "\tint features: 1\n",
      "\tfloat features: 1008\n",
      "\tobject features: 0\n",
      "\tData preprocessing and feature engineering runtime = 4.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: accuracy\n",
      "To change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will early stop models using evaluation metric: accuracy\n",
      "Fitting model: RandomForestClassifierGini_STACKER_l0 ...\n",
      "\t0.6041\t = Validation accuracy score\n",
      "\t184.3s\t = Training runtime\n",
      "\t3.59s\t = Validation runtime\n",
      "Fitting model: RandomForestClassifierEntr_STACKER_l0 ...\n",
      "\t0.6046\t = Validation accuracy score\n",
      "\t283.83s\t = Training runtime\n",
      "\t3.62s\t = Validation runtime\n",
      "Fitting model: ExtraTreesClassifierGini_STACKER_l0 ...\n",
      "\t0.6031\t = Validation accuracy score\n",
      "\t280.49s\t = Training runtime\n",
      "\t3.46s\t = Validation runtime\n",
      "Fitting model: ExtraTreesClassifierEntr_STACKER_l0 ...\n",
      "\t0.6002\t = Validation accuracy score\n",
      "\t267.11s\t = Training runtime\n",
      "\t4.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsClassifierUnif_STACKER_l0 ...\n",
      "\t0.451\t = Validation accuracy score\n",
      "\t116.24s\t = Training runtime\n",
      "\t44.99s\t = Validation runtime\n",
      "Fitting model: KNeighborsClassifierDist_STACKER_l0 ...\n",
      "\t0.4549\t = Validation accuracy score\n",
      "\t127.2s\t = Training runtime\n",
      "\t47.28s\t = Validation runtime\n",
      "Fitting model: LightGBMClassifier_STACKER_l0 ...\n",
      "\t0.6282\t = Validation accuracy score\n",
      "\t854.66s\t = Training runtime\n",
      "\t2.8s\t = Validation runtime\n",
      "Fitting model: CatboostClassifier_STACKER_l0 ...\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\tMany features detected (1009), dynamically setting 'colsample_bylevel' to 0.9910802775024777 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\t0.632\t = Validation accuracy score\n",
      "\t1314.42s\t = Training runtime\n",
      "\t1.58s\t = Validation runtime\n",
      "Fitting model: NeuralNetClassifier_STACKER_l0 ...\n",
      "\t0.5922\t = Validation accuracy score\n",
      "\t1555.84s\t = Training runtime\n",
      "\t11.64s\t = Validation runtime\n",
      "Fitting model: LightGBMClassifierCustom_STACKER_l0 ...\n"
     ]
    }
   ],
   "source": [
    "predictor = task.fit(train_data=train_data, label='bins', problem_type=\"multiclass\", auto_stack=True)\n",
    "performance = predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
